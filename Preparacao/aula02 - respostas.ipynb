{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unipinhal():\n",
    "    url = \"https://www.sou.unipinhal.edu.br/noticias\"\n",
    "    website = requests.get(url)\n",
    "    if website.status_code != 200:\n",
    "        return None\n",
    "    conteudo_website = website.content\n",
    "    website_analisado = BeautifulSoup(conteudo_website, 'html.parser')\n",
    "    noticias = website_analisado.find_all(\"div\", attrs={\"class\": \"_3tJ3x _1e-gz post-list-item-wrapper blog-post-homepage-description-font blog-post-homepage-description-color blog-post-homepage-description-fill _3RzkT\"})\n",
    "    noticias_completas = []\n",
    "    for noticia in noticias:\n",
    "        titulo = noticia.select('h2 div')[0].text\n",
    "        descricao = noticia.find('div', attrs={\"class\": \"_81XUh\"}).text\n",
    "        data = noticia.select('li span')[0].text\n",
    "        noticias_completas.append([titulo, descricao, data])\n",
    "    return noticias_completas\n",
    "unipinhal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Após a coleta dos dados vamos criar um **Dataframe** para armazenar esses dados em formato de tabela,e depois, exportar esses dados para uma tabela excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_dataframe(lista):\n",
    "    df = pd.DataFrame(lista, columns=['Título', 'descricao', 'data'])\n",
    "    print(df)\n",
    "    df.to_excel('noticiasUnipinhal.xlsx', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alibaba  resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_fornecedor_alibaba():\n",
    "    item = input('Digite o nome do produto: ')\n",
    "    numero_paginas = int(input('Digite o número de paginas: '))\n",
    "    url = f'https://www.alibaba.com/corporations/{item}.html?page=1'\n",
    "    # BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "    # ultima_pagina = BeautifulSoup(requests.get(url).content, 'html.parser').find_all('div', attrs={'class': 'ui2-pagination-pages'})\n",
    "    # ult_lista = [[ult.text] for ult in ultima_pagina]\n",
    "    # print(ult_lista)\n",
    "    lista_produtos = []\n",
    "    for i in range (1, numero_paginas + 1):\n",
    "        url = f'https://www.alibaba.com/corporations/{item}.html?page={i}'\n",
    "        print('pagina:', url)\n",
    "        website = requests.get(url)\n",
    "        if website.status_code != 200:\n",
    "            continue\n",
    "        website_content = website.content\n",
    "        website_parsed = BeautifulSoup(website_content, 'html.parser')\n",
    "        produtos = website_parsed.find_all(\"div\", attrs={\"class\": \"item-main\"})\n",
    "        for produto in produtos:\n",
    "            principais_produtos = produto.find(\"div\", attrs={\"class\": \"value ellipsis ph\"})\n",
    "            empresa = produto.find(\"a\", attrs={\"target\": \"_blank\"})\n",
    "            try:\n",
    "                lista_produtos.append([empresa.text, principais_produtos.text])\n",
    "            except Exception as e:\n",
    "                print(\"Algo errado. Erro:\", e)\n",
    "    pd.DataFrame(lista_produtos, columns = [\"Produto\", \"Preco\"]).to_excel('produtos.xlsx', index=False)\n",
    "busca_fornecedor_alibaba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unipinhal2():\n",
    "    noticias = website_parsed.find_all(\"span\", attrs={\"class\": \"_2PHJq public-DraftStyleDefault-ltr\"})\n",
    "    return [[noticia.text] for noticia in noticias]\n",
    "def unipinhal():\n",
    "    url = \"https://www.sou.unipinhal.edu.br/noticias\"\n",
    "    website = requests.get(url)\n",
    "    if website.status_code != 200:\n",
    "        return None\n",
    "    conteudo_website = website.content\n",
    "    website_analisado = BeautifulSoup(conteudo_website, 'html.parser')\n",
    "    noticias = website_analisado.find_all(\"div\", attrs={\"class\": \"_3tJ3x _1e-gz post-list-item-wrapper blog-post-homepage-description-font blog-post-homepage-description-color blog-post-homepage-description-fill _3RzkT\"})\n",
    "    noticias_completas = []\n",
    "    for noticia in noticias:\n",
    "        titulo = noticia.select('h2 div')[0].text\n",
    "        data = noticia.select('li span')[0].text\n",
    "        noticias_completas.append([titulo, descricao, data])\n",
    "    return noticias_completas\n",
    "unipinhal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta de dados de filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coleta_de_dados_de_filmes1():\n",
    "    url = \"https://www.imdb.com/find?s=tt&q=acao&ref_=nv_sr_sm\"\n",
    "    website = requests.get(url)\n",
    "    if website.status_code != 200:\n",
    "        return None\n",
    "    conteudo_website = website.content\n",
    "    website_analisado = BeautifulSoup(conteudo_website, 'html.parser')\n",
    "    informacoes = website_analisado.find_all(\"td\", attrs={\"class\": \"result_text\"})\n",
    "    informacoes_completas = []\n",
    "    for informacao in informacoes:\n",
    "        titulo = informacao.find('a').contents[0].text\n",
    "    #     descricao = informacao.find('div', attrs={\"class\": \"_81XUh\"}).text\n",
    "    #     data = informacao.select('li span')[0].text\n",
    "    #     informacoes_completas.append([titulo, descricao, data])\n",
    "        print(titulo)\n",
    "    # return informacoes_completas\n",
    "coleta_de_dados_de_filmes1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genero = input('Digite o genero do filme')\n",
    "url = f\"https://www.imdb.com/find?s=tt&q={genero}&ref_=nv_sr_sm\"\n",
    "website = requests.get(url)\n",
    "conteudo_website = website.content\n",
    "website_analisado = BeautifulSoup(conteudo_website, 'html.parser')\n",
    "titulos_filmes = [tag.find('a').contents[0] for tag in website_analisado.findAll('td', attrs={'class':'result_text'})]\n",
    "links_filmes = ['http://www.imdb.com' + tag.a['href'] for tag in website_analisado.findAll('td',  attrs={'class':'result_text'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filme exemplo para testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_exemplo = 'https://www.imdb.com/title/tt10481868/?ref_=fn_tt_tt_4'\n",
    "request_exemplo = requests.get(link_exemplo)\n",
    "soup = BeautifulSoup(request_exemplo.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando o score dos filmes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = soup.find('span', attrs ={'class':\"AggregateRatingButton__RatingScore-sc-1ll29m0-1 iTLWoV\"})\n",
    "print(score.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(soup):\n",
    "    score = soup.find('span', attrs ={'class':\"AggregateRatingButton__RatingScores-sc-1ll29m0-1 iTLWoV\"})\n",
    "    if score is None:\n",
    "        return None\n",
    "    else:\n",
    "        return float(score.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando numero de reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = soup.find('span', attrs ={'class':\"score\"})\n",
    "print(review.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qtdReviews(soup):\n",
    "    reviews = soup.find('span', attrs ={'class':\"score\"})\n",
    "    try:\n",
    "        for review in reviews:\n",
    "            if review.text.find('user') != -1:\n",
    "                qtd_r = review.text\n",
    "                print(review)\n",
    "                return int(qtd_r[:qtd_r.find(' user')])\n",
    "    except Exception:\n",
    "        return None\n",
    "get_qtdReviews(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando ano do filme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano = soup.find('span', attrs ={'class':\"TitleBlockMetaData__ListItemText-sc-12ein40-2 jedhex\"})\n",
    "print(ano.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ano(soup):\n",
    "    ano = soup.find('span', attrs ={'class':\"TitleBlockMetaData__ListItemText-sc-12ein40-2 jedhex\"})\n",
    "    try:\n",
    "        return int(ano.text[0:4])\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando a sinopse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinopse = soup.find('span', attrs ={'class':\"GenresAndPlot__TextContainerBreakpointXS_TO_M-cum89p-0 dcFkRD\"})\n",
    "print(sinopse.text.replace('... Read all', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinopse(soup):\n",
    "    sinopse = soup.find('span', attrs ={'class':\"GenresAndPlot__TextContainerBreakpointXS_TO_M-cum89p-0 dcFkRD\"})\n",
    "    try:\n",
    "        return sinopse.text.replace('... Read all', ' ')\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código com todas funcoes reunidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_filmes = []\n",
    "for titulo, link, i in zip(titulos_filmes, links_filmes, range(20)):\n",
    "    dic = {'titulo': titulo, 'link': link}\n",
    "    con = requests.get(link)\n",
    "    soup = BeautifulSoup(con.content, \"html.parser\")\n",
    "    con.close()\n",
    "\n",
    "    dic['sinopse'] = get_sinopse(soup)\n",
    "    dic['score'] = get_score(soup)\n",
    "    dic['ano'] = get_ano(soup)\n",
    "    dic['qtd_reviews'] = get_qtdReviews(soup)\n",
    "    # replace('None', 'Não informado')\n",
    "    lista_filmes.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos fazer uma wordCloud com as palavras da sinopse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenar as palavras\n",
    "all_summary = ''.join(dic['sinopse'] for dic in lista_filmes)\n",
    "\n",
    "# lista de stopword\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"da\", \"meu\", \"em\", \"você\", \"de\", \"ao\", \"os\"])\n",
    "\n",
    "# gerar uma wordcloud\n",
    "wordcloud = WordCloud(stopwords=stopwords,\n",
    "                      background_color=\"black\",\n",
    "                      width=1600, height=800).generate(all_summary)\n",
    "\n",
    "# mostrar a imagem final\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.imshow(wordcloud);\n",
    "wordcloud.to_file(\"airbnb_summary_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando a producao de filmes do genero a cada ano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma lista dos anos em que cada filme foi produzido\n",
    "anos  = [dic['ano'] for dic in lista_filmes if dic['ano'] != None]\n",
    "anos = set(anos)\n",
    "print(anos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo intervalos de tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'1938 - 1979': 0, '1980 - 1984':0, '1985 - 1989':0, '1990 - 1994':0, '1995 - 1999':0,\n",
    "      '2000 - 2004':0, '2005 - 2009':0, '2010 - 2017':0, '2018 - 2021':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando contagem dentro dos intervalos de ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chave in dic:\n",
    "    for filme in lista_filmes:\n",
    "        anos = chave.split()\n",
    "        anos.remove('-')\n",
    "        if (\n",
    "            filme['ano'] != None\n",
    "            and filme['ano'] >= int(anos[0])\n",
    "            and filme['ano'] <= int(anos[1])\n",
    "        ):\n",
    "            dic[chave] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando dados para **DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['intervalos_anos'] = dic.keys()\n",
    "df['freq'] = dic.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('intervalos_anos', 'freq', data=df, kind=\"bar\",palette=\"Greens\",size=6,aspect=2,legend_out=True);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "744e24d0957d42ad7b50819cc35ccf6cbf1ffdc9fa43baec5a1b56cdf1a86f6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
